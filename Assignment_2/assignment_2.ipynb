{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-21tlRxnfLW"
   },
   "source": [
    "# SNLP Assignment 2\n",
    "\n",
    "Name 1: Parnian Jahangirirad  \n",
    "Student id 1: 7062810  \n",
    "Email 1: paja00003@stud.uni-saarland.de  \n",
    " \n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook. There is no need to submit the data files. <br/>\n",
    "Upload the zipped folder on CMS. Please follow the naming convention of **Name1_studentID1_Name2_studentID2_Name3_studentID3.zip**. Make sure to click on \"Turn-in\" (or the equivalent on CMS) after you upload your submission, otherwise the assignment will not be considered as submitted. Only one member of the group should make the submisssion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0o_hqMzc7qAH"
   },
   "source": [
    "## Exercise 1 - Probability theory (0.5 + 0.5 = 1 point)\n",
    "\n",
    "Consider a fair 6-sided die whose sides are numbered from 1 to 6 and each die roll is independent of the other rolls. In an experiment that consists of rolling the die twice, the following events can be defined:\n",
    "\n",
    "    A: The sum of the two outcomes is at least 10\n",
    "    B: At least one of the two rolls resulted in 6\n",
    "\n",
    "a. Compute the probabilities $P(A)$, and $P(B)$.\n",
    "\n",
    "b. Is event A independent of event B?\n",
    "\n",
    "You can leave the solution as a fraction.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution 1  \n",
    "##### a:\n",
    "- $P(A)=\\frac{1}{6}$  \n",
    "  \n",
    "- $P(B) = \\frac{11}{36}$  \n",
    "  \n",
    "##### B:  \n",
    "Two events are independent when we have\n",
    "$P(AB) = P(A).P(B)$  \n",
    "we have:  \n",
    "$P(AB) = \\frac{5}{36} \\neq P(A).P(B) = \\frac{11}{216}$  \n",
    "So this 2 events are not independent.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Bayes theorem (0.5 + 1 = 1.5 points)\n",
    "\n",
    "Bayes' theorem is a fundamental concept in probability theory used to update the probability of a hypothesis based on new evidence. It states that the probability of a hypothesis (H) given some evidence (E) is proportional to the probability of the evidence given the hypothesis, multiplied by the prior probability of the hypothesis, divided by the probability of the evidence regardless of the hypothesis. Mathematically,\n",
    "\n",
    "$$ P(H|E) = \\frac{P(E|H)P(H)}{P(E)} $$\n",
    "\n",
    "Where,\n",
    "- $P(H∣E)$ is the posterior probability of hypothesis H given evidence E.\n",
    "- $P(E∣H)$ is the probability of observing evidence E given that the hypothesis H is true.\n",
    "- $P(H)$ is the prior probability of hypothesis H being true.\n",
    "- $P(E)$ is the probability of observing evidence E.\n",
    "\n",
    "\n",
    "Now, Imagine that all of the blogs online were generated by some Large Language Model (LLM). Of these blogs, $80\\%$ were generated using GPT-4, $15\\%$ of them were generated using LLAMA 3 and $5\\%$ were generated using Mistral. About $4\\%$ of the text generated by GPT-4 seem to have counterfactual statements. For LLAMA 3, this is about $6\\%$ and for Mistral, this is about $9\\%$.\n",
    "\n",
    "a. If a blog post is randomly selected from these online blogs, what is the probability that it was generated by LLAMA 3?\n",
    "\n",
    "b. If a randomly selected blog post contained a counterfactual statement, find the probability that it was generated using GPT-4.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution 2  \n",
    "##### a:  \n",
    "We know that 15% of post blogs were generated using LLAMA3, so the probability of choosing a random blog that is generated by LLAMA3 is 15%.  \n",
    "  \n",
    "##### b:  \n",
    "$E$ : The chosen blog has a counterfactual statement.  \n",
    "$H$ : The chosen blog is generated by GPT4.  \n",
    "$P(H|E) = \\frac{P(E|H) P(H)}{P(E)} = \\frac{4*80}{4*80 + 6*15 + 9*5} \\approx 70.33%$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Miller's model (0.5 + 1 + 0.5 = 2 points)\n",
    "\n",
    "Miller's model introduced in the lecture simulates the process of random text generation at the character level. It is a memoryless model which means that the generation at each time step is independent of what has been generated so far. Imagine that there is a monkey typing on a computer keyboard. The keyboard has only 27 keys: a to z and a spacebar. Assume that the probability of each character is distributed according to its occurrence in the English language. We call a sequence of characters separated by white space as a word.\n",
    "\n",
    "a. What is the probability that the monkey will type the word _'ear'_ on the keyboard? Hint: See the `exercise_3.py` file.\n",
    "\n",
    "b. Complete the `compute_perplexity()` function in the `exercise_3.py` file to compute the perplexity of the Miller's model for the text: \"hello i am a monkey\". \n",
    "\n",
    "c. What would happen to the perplexity if we were to add a few more whitespace in between the words?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution 3  \n",
    "##### a:  \n",
    "$P(ear) = P(e)*P(a)*P(r) = 0.08929 * 0.06798 * 0.06065 \\approx 0.000366$  \n",
    "  \n",
    "##### c:  \n",
    "In order to check the effect of adding more spaces between words, we will compare the perplexity of sentence \"hello i am a monkey\" with sentence \"hello  i  am  a  monkey\". The first sentence has a perplexity approximately equal to 1.2929, but for the second sentence, the perplexity is almost 1.3404. So it seems that adding more spaces between words can increase perplexity.  \n",
    "This happens because the probability of \" \" is relatively high compared to the probability of other characters. So adding more spaces can increase the entropy, and respectively the perplexity, which is $exp(entropy)$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  1.292949309151515\n"
     ]
    }
   ],
   "source": [
    "# Solution 3.b\n",
    "\n",
    "# DO NOT MODIFY THIS PART\n",
    "# Your code should be placed in exercise_3.py\n",
    "import exercise_3\n",
    "from importlib import reload\n",
    "\n",
    "reload(exercise_3)\n",
    "\n",
    "model = exercise_3.MillersModel()\n",
    "\n",
    "text = \"hello i am a monkey\"\n",
    "\n",
    "print(\"Perplexity: \", model.compute_perplexity(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  1.3404506279763355\n"
     ]
    }
   ],
   "source": [
    "import exercise_3\n",
    "from importlib import reload\n",
    "\n",
    "reload(exercise_3)\n",
    "\n",
    "model = exercise_3.MillersModel()\n",
    "\n",
    "text = \"hello  i  am  a  monkey\"\n",
    "\n",
    "print(\"Perplexity: \", model.compute_perplexity(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - Sampling in Language models (1 + 2 + 0.5 + 0.5 =4 points)\n",
    "\n",
    "Neural Networks are quite effective as generative models. Huggingface Transformers library can be used to easily get started with training and using these models. Take a look at the [Huggingface transformers quick tour](https://huggingface.co/docs/transformers/quicktour) to get acquainted with the transformers library. (Recommended: [Chapter 1](https://huggingface.co/learn/nlp-course/chapter1/1) and [Chapter 2](https://huggingface.co/learn/nlp-course/chapter2/1) of the Huggingface NLP Course)\n",
    "\n",
    "a. Read up on GPT2 [[1]](https://huggingface.co/openai-community/gpt2) [[2]](https://huggingface.co/docs/transformers/model_doc/gpt2). Implement the `forward()` function in the `GPT2Model` class in `exercise_4.py` file to take input text sequence and return the probability distribution of the predicted next token. The general steps would be: tokenize and encode the input text, feed the input to the model to get the logits (Note: You might have to take only the last logit which is for the predicted next token in the sequence.), and apply softmax function to convert it to a probability distribution.\n",
    "\n",
    "b. We can use different types of sampling strategies while selecting the next token for generating a text. Some common approaches are: Greedy sampling, random sampling, beam search, etc.\n",
    "\n",
    "   - Greedy sampling\n",
    "      In greedy sampling, the model selects the word with the highest probability based on the given context. It prioritizes the most likely next word based on the model's predictions, without considering other possibilities. This method often produces coherent and fluent text but can lead to repetitive or predictable outputs, as it doesn't explore alternative word choices.\n",
    "\n",
    "   - Random sampling\n",
    "      In random sampling, the next token is randomly sampled from the output probability distribution. This can make the generation non-deterministic and unpredictable but doesn't usually fall into repetitions.\n",
    "\n",
    "   Implement the `greedy_sample()` and `random_sample()` functions in `GPT2Model` class in `exercise_4.py` file for Greedy sampling and random sampling respectively. For each of them, generate a continuation of the sentence given below.\n",
    "\n",
    "c. Describe the output with each sampling method. Which one would you generally prefer?  \n",
    "- The output of greedy sampling method seems quite repititive. This is bacuase the greedy sampling method chooses the token with the highest probability at each step.  \n",
    "- The output of random sampling is more varied compared to the output of greedy sampling, because random sampling selects the next token based on the probability distribution of tokens. This approach results in a less repititve result compared to the greedy sampling approach.  \n",
    "\n",
    "The output of ransom sampling seems to be a better choice because it is less repititve and more diverse.  \n",
    "\n",
    "d. Another approach for generating from language models is using Beam search. Describe how the beam search works (2 sentences). Does it solve any problems of the previous sampling methods?  \n",
    "- Beam search explores several states(possible tokens) at the same time. This qpproach expands each beam by considering all possible next tokens, and keeps the best b beams(b:number of beams) based on the cumulative probabilities until the sequence is completed.  \n",
    "Beam search can improve greedy sampling, because it considers multiple possible sequences instead of selecting the token with highest probability at each step. This modification will result in a less repititive output.  \n",
    "Beam search can also improve ransom sampling by focusing on the first b tokens with the highest probability, instead of just using the next token based on the probability distribution. This modification can result in a less random, more coherent output.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (2.1.1)\n",
      "Collecting torch\n",
      "  Using cached torch-2.3.0-cp311-cp311-win_amd64.whl (159.8 MB)\n",
      "Requirement already satisfied: evaluate in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (0.4.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (2.19.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: boto3 in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from transformers) (1.24.28)\n",
      "Requirement already satisfied: requests in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from transformers) (2.29.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: regex in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Collecting sentencepiece (from transformers)\n",
      "  Using cached sentencepiece-0.2.0-cp311-cp311-win_amd64.whl (991 kB)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: filelock in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Using cached typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: sympy in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Collecting mkl<=2021.4.0,>=2021.1.1 (from torch)\n",
      "  Using cached mkl-2021.4.0-py2.py3-none-win_amd64.whl (228.5 MB)\n",
      "Requirement already satisfied: dill in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from evaluate) (1.5.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from evaluate) (0.23.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from evaluate) (23.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Collecting intel-openmp==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch)\n",
      "  Using cached intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl (3.5 MB)\n",
      "Collecting tbb==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch)\n",
      "  Using cached tbb-2021.12.0-py3-none-win_amd64.whl (286 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from tqdm->transformers) (0.4.6)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.28 in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from boto3->transformers) (1.27.59)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from boto3->transformers) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from boto3->transformers) (0.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2022.7)\n",
      "Requirement already satisfied: six in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\pjrad\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Installing collected packages: tbb, sentencepiece, intel-openmp, typing-extensions, mkl, torch\n",
      "  Attempting uninstall: tbb\n",
      "    Found existing installation: TBB 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot uninstall 'TBB'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\n"
     ]
    }
   ],
   "source": [
    "# install necessary packages\n",
    "!pip install transformers torch evaluate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pjrad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Solution 4\n",
    "\n",
    "# DO NOT MODIFY THIS PART\n",
    "# Your code should be placed in exercise_4.py\n",
    "import exercise_4\n",
    "from importlib import reload\n",
    "\n",
    "reload(exercise_4)\n",
    "\n",
    "model = exercise_4.GPT2Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy sampled output:  Alice was beginning to get very tired of sitting by her sister on the bank   and she was starting to get tired of sitting\n"
     ]
    }
   ],
   "source": [
    "# Greedy sampling\n",
    "n_tokens = 10\n",
    "\n",
    "text = \"Alice was beginning to get very tired of sitting by her sister on the bank \"\n",
    "\n",
    "for _ in range(n_tokens):\n",
    "    text += model.greedy_sample(text)\n",
    "\n",
    "print(\"Greedy sampled output: \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sampled output:  Alice was beginning to get very tired of sitting by her sister on the bank   tomorrow. And she felt very enraged. She\n"
     ]
    }
   ],
   "source": [
    "# Random sampling\n",
    "n_tokens = 10\n",
    "\n",
    "text = \"Alice was beginning to get very tired of sitting by her sister on the bank \"\n",
    "\n",
    "for _ in range(n_tokens):\n",
    "    text += model.random_sample(text)\n",
    "\n",
    "print(\"Random sampled output: \", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 - Computing Perplexity (0.5 + 1 = 1.5 points)\n",
    "\n",
    "Huggingface provides the `evaluate` library that includes various metrics used in machine learning along with perplexity that we use in Language modeling. We can use that library to quickly compute the perplexity of a language model. There is also the `datasets` library from Huggingface that allows to easily access datasets for many different tasks.\n",
    "\n",
    "a. Read the [metric card for perplexity](https://huggingface.co/spaces/evaluate-metric/perplexity) and compute the perplexity for the following text using GPT2.\n",
    "\n",
    "b. Load the `alice_in_wonderland.txt` file using Hugginface `datasets` library and compute the perplexity for it using GPT2. Check the [documentations](https://huggingface.co/docs/datasets/index) for datasets library.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.23.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.19.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.21.1)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.40.2-py3-none-any.whl (9.0 MB)\n",
      "Requirement already satisfied: evaluate in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.4.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface_hub) (3.7.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface_hub) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface_hub) (20.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface_hub) (2.25.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface_hub) (4.64.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface_hub) (4.2.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (1.20.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (1.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (20.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=20.9->huggingface_hub) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface_hub) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface_hub) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface_hub) (1.26.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface_hub) (2020.12.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\pjrad\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pjrad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.21.1\n",
      "    Uninstalling transformers-4.21.1:\n",
      "      Successfully uninstalled transformers-4.21.1\n",
      "Successfully installed transformers-4.40.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade huggingface_hub datasets transformers evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe6ee11ac5b4bce92ecc274de0b49fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  33.54\n"
     ]
    }
   ],
   "source": [
    "# Solution 5.a\n",
    "from evaluate import load\n",
    "\n",
    "texts = [\"Eureka! I have found it!\"]\n",
    "\n",
    "# your code here\n",
    "perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "result = perplexity.compute(model_id='gpt2',\n",
    "                             add_start_token=False,\n",
    "                             predictions=texts)\n",
    "\n",
    "print(\"Perplexity: \",round(result[\"mean_perplexity\"], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pjrad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe19c7b0f5f4be282b355cd6b29e245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['perplexities', 'mean_perplexity']\n",
      "Perplexity: 458.96\n"
     ]
    }
   ],
   "source": [
    "# Solution 5.b\n",
    "from evaluate import load\n",
    "from datasets import load_dataset\n",
    "\n",
    "# your code here\n",
    "dataset = load_dataset('text', data_files='alice_in_wonderland.txt')['train']\n",
    "\n",
    "# Extract text from the dataset, removing texts that are too short\n",
    "texts = [example['text'] for example in dataset if len(example['text'].split()) > 1]\n",
    "\n",
    "# Load the perplexity metric\n",
    "perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "\n",
    "# Compute perplexity\n",
    "result = perplexity.compute(model_id='gpt2',\n",
    "                            add_start_token=False,\n",
    "                            predictions=texts)\n",
    "\n",
    "print(list(result.keys()))\n",
    "# Print the perplexity value\n",
    "print(\"Perplexity:\", round(result['mean_perplexity'],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rejection Sampling (2 points)\n",
    "\n",
    "\n",
    "Rejection sampling is another approach we can use to generate the next token from the language model. In this approach, we randomly sample a token from the list and accept it if it's probability is greater than some random number and reject it otherwise.\n",
    "\n",
    "1. Generate a random number, r.\n",
    "2. Randomly sample a token from the output probability (say token at index i with probability p).\n",
    "3. Is r < p?\n",
    "\n",
    "    a. If yes, then accept i as the next token.\n",
    "    \n",
    "    b. If no, then go back to step 1.\n",
    "\n",
    "Complete the `rejection_sample()` function in the `GPT2Model` class in `exercise_4.py`. How does the generated output differ from the greedy and random sampling?\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejection sampled output:  Alice was beginning to get very tired of sitting by her sister on the bank   and she decided to go to the local mall\n"
     ]
    }
   ],
   "source": [
    "# Rejection sampling\n",
    "import exercise_4\n",
    "n_tokens = 10\n",
    "\n",
    "text = \"Alice was beginning to get very tired of sitting by her sister on the bank \"\n",
    "\n",
    "for _ in range(n_tokens):\n",
    "    text += model.rejection_sample(text)\n",
    "\n",
    "print(\"Rejection sampled output: \", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of rejection sampling is less repititve comparing to greedy sampling, and more coherent (the generated text seems less random) comparing to random sampling.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity with different languages (0.5 + 0.5 = 1 point)\n",
    "\n",
    "a. In exercise 5.a., what would happen if we were to use a different language other than English? How would the perplexity be affected? Test this with another language of your choice.\n",
    "\n",
    "b. Are there any pre-trained model for the language you selected? Try with that model to see if you can get a better perplexity score. If there are no pre-trained LM for that language, try with a multilingual LM and see if the perplexity score improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pjrad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c313eb08ff524acbbf1203097d9e4abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  94.67\n"
     ]
    }
   ],
   "source": [
    "#5.1 with German language\n",
    "from evaluate import load\n",
    "\n",
    "texts = [\"Ja! Ich habe das gefunden.\"]\n",
    "\n",
    "# your code here\n",
    "perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "result = perplexity.compute(model_id='gpt2',\n",
    "                             add_start_token=False,\n",
    "                             predictions=texts)\n",
    "\n",
    "print(\"Perplexity: \",round(result[\"mean_perplexity\"], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the perplexity highly increased for a sentence in German."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pjrad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f49e77f3db4676a33f6ef13af63322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/865 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pjrad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pjrad\\.cache\\huggingface\\hub\\models--dbmdz--german-gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b243241b9028438d80991e708b117f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/510M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "766ec3c6b1de4c9bbf36fe5b4b23c5b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.43M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd21d861cda246f3a07b271755f54072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  61.58\n"
     ]
    }
   ],
   "source": [
    "# try a new pretrained model \"German GPT-2\"\n",
    "from evaluate import load\n",
    "\n",
    "texts = [\"Ja! Ich habe das gefunden.\"]\n",
    "\n",
    "# your code here\n",
    "perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "result = perplexity.compute(model_id=\"dbmdz/german-gpt2\",\n",
    "                             add_start_token=False,\n",
    "                             predictions=texts)\n",
    "\n",
    "print(\"Perplexity: \",round(result[\"mean_perplexity\"], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, using a German pre-trained language model resulted in an improvment in the perplexity.  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
